# Database Configuration
DATABASE_URL="file:./dev.db"

# Local LLM Configuration
LLM_PROVIDER="lmstudio"  # Options: "lmstudio" or "ollama"
LLM_BASE_URL="http://127.0.0.1:1234"  # LM Studio default
# LLM_BASE_URL="http://localhost:11434"  # Ollama default
LLM_MODEL="google/gemma-3-4b"  # Fast, small model (~355MB)
LLM_TEMPERATURE="0.5"  # Lower temperature = faster, more focused responses
LLM_MAX_TOKENS="512"   # Shorter responses = faster generation

# Application Configuration
NODE_ENV="development"
NEXT_PUBLIC_APP_NAME="Zara AI Companion"
NEXT_PUBLIC_APP_VERSION="0.1.0"

# Security (for future use)
JWT_SECRET="your-secret-key-here"
ENCRYPTION_KEY="your-encryption-key-here"

# Optional: External Services (for future features)
# OPENAI_API_KEY=""
# ANTHROPIC_API_KEY=""
# ELEVENLABS_API_KEY=""

# Development Settings
DEBUG="true"
LOG_LEVEL="info"

# LM Studio Performance Settings (for faster responses)
# These are suggestions for LM Studio configuration:
# - Context Length: 1024 (instead of 4096)
# - GPU Offload: 30/40 layers
# - CPU Thread Pool: 6-8 threads
# - Evaluation Batch Size: 256
# - Flash Attention: ON (if available)
